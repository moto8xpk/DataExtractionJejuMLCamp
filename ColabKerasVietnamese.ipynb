{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ColabKerasVietnamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ltk2_4JRt1da",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import pdb;\n",
        "from time import time\n",
        "import glob, shutil\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.layers import SeparableConv2D,ReLU,CuDNNGRU, add,MaxPooling2D , Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional,Activation\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,TensorBoard,EarlyStopping\n",
        "from keras.layers.recurrent import GRU\n",
        "import tensorflow as tf\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bknxwOd7zALN",
        "colab_type": "text"
      },
      "source": [
        "### Import all necessary  library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuItcNSzlZ0",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive to Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "blyocJdDeqcS",
        "outputId": "0621c55f-8481-48c3-9743-d17c6a0d8fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kfto-8Kee1XD",
        "outputId": "1a21cfeb-eab4-46a6-917d-96c56291521b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/'My Drive/Vietnamese'"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Vietnamese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2XNRLIstuBm-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0uJw0qmauNFm",
        "outputId": "3e4eb36e-4a92-4540-cd97-2611bdce8085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "TF_CONFIG_=tf.ConfigProto()\n",
        "TF_CONFIG_.allow_soft_placement=True\n",
        "TF_CONFIG_.gpu_options.per_process_gpu_memory_fraction=0.4\n",
        "TF_CONFIG_.log_device_placement=True\n",
        "sess = tf.Session(config=TF_CONFIG_)\n",
        "# sess.run(init)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13396373273913228708\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2997015954750289465\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 9614792480319192142\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11326753997\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 15739511965698922232\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnilpmUFu3xD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5da29ef9-13b2-454c-91da-ce2a56846e47"
      },
      "source": [
        "char_list=' !\"#&\\'()*+,-./0123456789:;?AÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶBCDĐEÉÈẺẼẸÊẾỀỂỄỆFGHIÍÌỈĨỊJKLMNOÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢPQRSTUÚÙỦŨỤỨỪỬỮỰVWXYÝỲỶỸỴZaáàảãạâấầẩẫậăắằẳẵặbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz'\n",
        "print(char_list)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " !\"#&'()*+,-./0123456789:;?AÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶBCDĐEÉÈẺẼẸÊẾỀỂỄỆFGHIÍÌỈĨỊJKLMNOÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢPQRSTUÚÙỦŨỤỨỪỬỮỰVWXYÝỲỶỸỴZaáàảãạâấầẩẫậăắằẳẵặbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCLxu9lbz7VH",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the Label to number array\n",
        "\n",
        "> By the Charmap above include the Latin, Vietnamese character and symbol (with the **Blank** too): \n",
        "\n",
        "!\"#&\\'()*+,-./0123456789:;?AÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶBCDĐEÉÈẺẼẸÊẾỀỂỄỆFGHIÍÌỈĨỊJKLMNOÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢPQRSTUÚÙỦŨỤỨỪỬỮỰVWXYÝỲỶỸỴZaáàảãạâấầẩẫậăắằẳẵặbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDGHYOQ4v7kX",
        "colab": {}
      },
      "source": [
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "#             print(dig_lst)\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz_ESV7x3NHM",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing image\n",
        "First I generated the data with this tool that I customize a little in [this repo](https://github.com/moto8xpk/TextRecognitionDataGenerator)\n",
        "* Before this, I use some script to modify the size of data to (280,32)\n",
        "\n",
        "Use opencv to read image and convert to black white image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSLKz9hwwM37",
        "colab": {}
      },
      "source": [
        "# lists for training dataset\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        " \n",
        "max_label_len = 0\n",
        "\n",
        "i =1 \n",
        "flag = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LK3ABMJkwZsk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a8de87c-f97a-443c-bc64-371acf03640d"
      },
      "source": [
        "# for root , directory name and file name\n",
        "for root, dirnames, filenames in os.walk('./src/training/out-pre'):\n",
        "    # preprocessing image\n",
        "    for f_name in fnmatch.filter(filenames, '*.jpg'):\n",
        "        # read input image and convert into gray scale image\n",
        "        stream = open(os.path.join(root, f_name),\"rb\")\n",
        "        bytes = bytearray(stream.read())\n",
        "        numpyarray = np.asarray(bytes, dtype=np.uint8)\n",
        "        img = cv2.imdecode(numpyarray, cv2.IMREAD_UNCHANGED)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        # Convert gray scale image to black and white image\n",
        "        (thresh, img) = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "#         image_input_show = plt.imshow(img)  \n",
        "#         plt.show()\n",
        "        # convert each image of shape (32, 128, 1)\n",
        "        w , h = img.shape\n",
        "        \n",
        "        if h > 280 or w > 32:\n",
        "            continue\n",
        "        if w < 32:\n",
        "            add_zeros = np.ones((32-w, h))*255\n",
        "            img = np.concatenate((img, add_zeros))\n",
        " \n",
        "        if h < 280:\n",
        "            add_zeros = np.ones((32,280-h))*255\n",
        "            img = np.concatenate((img, add_zeros), axis=1)\n",
        "        img = np.expand_dims(img , axis = 2)\n",
        "        \n",
        "        # Normalize each image\n",
        "#         img = img/255.\n",
        "\n",
        "        # get the text from the image\n",
        "        txt = f_name.split('_')[0]\n",
        "\n",
        "        # compute maximum length of the text\n",
        "        if len(txt) > max_label_len:\n",
        "            max_label_len = len(txt)\n",
        "           \n",
        "        # split the 10000 data into validation and training dataset as 10% and 90% respectively\n",
        "        if i%4 == 0:     \n",
        "            valid_orig_txt.append(txt)   \n",
        "            valid_label_length.append(len(txt))\n",
        "            valid_input_length.append(67)\n",
        "            valid_img.append(img)\n",
        "            valid_txt.append(encode_to_labels(txt))\n",
        "        else:\n",
        "            orig_txt.append(txt)   \n",
        "            train_label_length.append(len(txt))\n",
        "            train_input_length.append(67)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(txt))\n",
        "           \n",
        "        # break the loop if total data is 15000\n",
        "        if i == 15000:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1\n",
        "    if flag == 1:\n",
        "        break\n",
        "\n",
        "print(len(training_img))\n",
        "print(len(valid_img))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "̀\n",
            "́\n",
            "́\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "́\n",
            "̉\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "̀\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "̂\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "́\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "̂\n",
            "́\n",
            "̣\n",
            "̂\n",
            "8304\n",
            "2768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKBZIVtOebqT",
        "colab": {}
      },
      "source": [
        "# pad each output label to maximum text length\n",
        " \n",
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "\n",
        "# input with shape of height=32 and width=280 \n",
        "inputs = Input(shape=(32,280,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DU0ik-uSD9R",
        "colab_type": "text"
      },
      "source": [
        "### Create The Architechtur of CRNN\n",
        "This network architecture is inspired by this [paper](https://arxiv.org/pdf/1507.05717.pdf).\n",
        "\n",
        "1.   Input shape for our architecture having an input image of height 32 and width 280. \n",
        "2.   Here we used 7 convolution layers of which 6 are having kernel size (3,3) and the last one is of size (2.2). And the number of filters is increased from 64 to 512 layer by layer.\n",
        "3.   Two max-pooling layers are added with size (2,2) and then two max-pooling layers of size (2,1) are added to extract features with a larger width to predict long texts.\n",
        "4.   Also, we used batch normalization layers after fifth and sixth convolution layers which accelerates the training process.\n",
        "5.   Then we used a lambda function to squeeze the output from conv layer and make it compatible with GRU layer.\n",
        "6.    Then used two Bidirectional GRU layers each of which has 128 units. This RNN layer gives the output of size (batch_size, 31, 221). Where 63 is the total number of output classes including blank character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fM9cXJXVxB6W",
        "colab": {}
      },
      "source": [
        "\n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        "\n",
        "\n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "# bidirectional GRU layers with units=128\n",
        "blstm_1 = Bidirectional(GRU(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(GRU(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "# act_model = Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwShwv7LxL__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "outputId": "8d1e7fc9-c2fe-4fcf-fd63-4f9f685d57c9"
      },
      "source": [
        "# act_model.summary()\n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 32, 280, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 280, 64)  640         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 16, 140, 64)  0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 140, 128) 73856       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 8, 70, 128)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 70, 256)   295168      max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 70, 256)   590080      conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 4, 70, 256)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 70, 512)   1180160     max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 4, 70, 512)   2048        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 70, 512)   2359808     batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 70, 512)   2048        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 2, 70, 512)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 1, 69, 512)   1049088     max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 69, 512)      0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 69, 256)      492288      lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 69, 256)      295680      bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 69, 213)      54741       bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "the_labels (InputLayer)         (None, 29)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_length (InputLayer)       (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "label_length (InputLayer)       (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ctc (Lambda)                    (None, 1)            0           dense_3[0][0]                    \n",
            "                                                                 the_labels[0][0]                 \n",
            "                                                                 input_length[0][0]               \n",
            "                                                                 label_length[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 6,395,605\n",
            "Trainable params: 6,393,557\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbcmlQZpUy49",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Loss Function\n",
        "Here, we are using the CTC loss function. CTC loss is very helpful in text recognition problems. It helps us to prevent annotating each time step and help us to get rid of the problem where a single character can span multiple time step which needs further processing if we do not use CTC. If you want to know more about CTC( Connectionist Temporal Classification ) please follow [this paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf).\n",
        "\n",
        "A CTC loss function requires four arguments to compute the loss, predicted outputs, ground truth labels, input sequence length to LSTM and ground truth label length. To get this we need to create a custom loss function and then pass it to the model. To make it compatible with our model, we will create a model which takes these four inputs and outputs the loss. This model will be used for training and for testing we will use the model that we have created earlier “act_model”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TdvRZKsbxPlH",
        "colab": {}
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BbnfQXkWebql",
        "outputId": "12f6c5b2-bbda-4508-df95-f3017d62b5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(inputs)\n",
        "print(input_length)\n",
        "print(labels)\n",
        "print(label_length)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_3:0\", shape=(?, 32, 280, 1), dtype=float32)\n",
            "Tensor(\"input_length_2:0\", shape=(?, 1), dtype=int64)\n",
            "Tensor(\"the_labels_2:0\", shape=(?, 29), dtype=float32)\n",
            "Tensor(\"label_length_2:0\", shape=(?, 1), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44eN48a9YcBQ",
        "colab_type": "text"
      },
      "source": [
        "## Configuration of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0iTNjSjPebqq",
        "colab": {}
      },
      "source": [
        "# opt=RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# opt = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "opt= SGD(lr=0.001,decay=1e-6,momentum=0.9,nesterov=True,clipnorm=5)\n",
        "# opt=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-9,\n",
        "#                  decay=1e-9, amsgrad=True, clipnorm=5., clipvalue=0.5)\n",
        "reduce_lr = ReduceLROnPlateau(patience=6, verbose=1, facttor=0.75)\n",
        "# tensorboard=TensorBoard(log_dir='./logs-keras/{}'.format(time()), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
        "earlyStop=EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e8JP_gBQ1Iq5",
        "colab": {}
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = opt,metrics=['accuracy'])\n",
        " \n",
        "filepath=\"./best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint,reduce_lr,earlyStop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQdJLSlP1X0x",
        "colab": {}
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqhpynydW_xj",
        "colab_type": "text"
      },
      "source": [
        "## Load your trained model to continue the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WEGX11lNebq1",
        "colab": {}
      },
      "source": [
        "# model.load_weights('./best_model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYwS4HaPXJHA",
        "colab_type": "text"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Io7JzPg1azY",
        "outputId": "fd888908-c405-4e76-f2d4-9bd1ccb9ec5e",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "batch_size = 60\n",
        "epochs = 50\n",
        "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], \n",
        "          y=np.zeros(len(training_img)), \n",
        "          batch_size=batch_size, \n",
        "          epochs = epochs, \n",
        "          validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], \n",
        "          [np.zeros(len(valid_img))]), \n",
        "          verbose = 1, \n",
        "          callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8304 samples, validate on 2768 samples\n",
            "Epoch 1/50\n",
            "8304/8304 [==============================] - 125s 15ms/step - loss: 54.3621 - acc: 0.0000e+00 - val_loss: 28.9075 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 28.90753, saving model to ./best_model.hdf5\n",
            "Epoch 2/50\n",
            "8304/8304 [==============================] - 115s 14ms/step - loss: 26.8784 - acc: 0.0000e+00 - val_loss: 29.0128 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 28.90753\n",
            "Epoch 3/50\n",
            "8304/8304 [==============================] - 115s 14ms/step - loss: 22.4747 - acc: 0.0000e+00 - val_loss: 20.4261 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss improved from 28.90753 to 20.42612, saving model to ./best_model.hdf5\n",
            "Epoch 4/50\n",
            "8304/8304 [==============================] - 115s 14ms/step - loss: 16.5019 - acc: 0.0000e+00 - val_loss: 14.6328 - val_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss improved from 20.42612 to 14.63278, saving model to ./best_model.hdf5\n",
            "Epoch 5/50\n",
            "1080/8304 [==>...........................] - ETA: 1:29 - loss: 12.8143 - acc: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Wf8xPMYjFY",
        "colab_type": "text"
      },
      "source": [
        "## Testing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7iAv1bHt1qNC",
        "colab": {}
      },
      "source": [
        "act_model = Model(inputs, outputs)\n",
        "# # load the saved best model weights\n",
        "act_model.load_weights('./best_model.hdf5')\n",
        "# # predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img)\n",
        " \n",
        "# # use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# # see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"image = \")\n",
        "    print(\"original_text =  \", valid_orig_txt[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMAhYD4zYoXt",
        "colab_type": "text"
      },
      "source": [
        "## Infer the test\n",
        "when I run the crop image srcipt this will be the Python script that I will create as a API service on my server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bvx08bOANUVH",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras import *\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import SeparableConv2D,ReLU,CuDNNGRU, add,MaxPooling2D , Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional,Activation\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "from keras.models import Model\n",
        "char_list=' !\"#&\\'()*+,-./0123456789:;?AÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶBCDĐEÉÈẺẼẸÊẾỀỂỄỆFGHIÍÌỈĨỊJKLMNOÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢPQRSTUÚÙỦŨỤỨỪỬỮỰVWXYÝỲỶỸỴZaáàảãạâấầẩẫậăắằẳẵặbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz'\n",
        "\n",
        "inputs = Input(shape=(32,280,1))\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        "\n",
        "\n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(GRU(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(GRU(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(212+1, activation = 'softmax')(blstm_2) #char list len is 212\n",
        "\n",
        "inference_model=Model(inputs, outputs)\n",
        "inference_model.load_weights('./best_model.hdf5')\n",
        "\n",
        "stream = open('./img37.png',\"rb\")\n",
        "bytes = bytearray(stream.read())\n",
        "numpyarray = np.asarray(bytes, dtype=np.uint8)\n",
        "img = cv2.imdecode(numpyarray, cv2.IMREAD_UNCHANGED)\n",
        "print(img.shape)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "(thresh, img) = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "# cv2.imshow('image',img)\n",
        "image_input_show = plt.imshow(img)  \n",
        "plt.show()\n",
        "w , h = img.shape\n",
        "if w < 32:\n",
        "    add_zeros = np.ones((32-w, h))*255\n",
        "    img = np.concatenate((img, add_zeros))\n",
        "\n",
        "if h < 280:\n",
        "    add_zeros = np.ones((32,280-h))*255\n",
        "    img = np.concatenate((img, add_zeros), axis=1)\n",
        "img = np.expand_dims(img , axis = 2)\n",
        "img = np.expand_dims(img , axis = 0)\n",
        "print(img.shape)\n",
        "# img = img/255.0\n",
        "\n",
        "# print(img)\n",
        "\n",
        "preds = inference_model.predict(img)\n",
        "out = K.get_value(K.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "i = 0\n",
        "for x in out:\n",
        "  # print(\"original_text =  \", valid_orig_txt[i])\n",
        "  print(\"predicted text = \", end = '')\n",
        "  for p in x:  \n",
        "      if int(p) != -1:\n",
        "          print(char_list[int(p)], end = '')       \n",
        "  print('\\n')\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNLNuo1EZRDA",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "\n",
        "1.   First of all, this was the first project that I have learn about the text-based extraction so I have a lot of problem when I deal with dataset and create architecture of the neural network of Convolution Recurrent neural network\n",
        "2.   In my project, the dataset I use for training is not suitable for extraction data of the Medical card because  my model predict pretty well with my data that I generated from the tool\n",
        "3.   In further, I will test some VGG16 or Resnet to estimate the accuracy of the model and I will collect more data of the real form of medical record to extract.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWIFPofPnuKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}